import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.Sources;

import java.util.Random;
import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    private static final String[] NAMES = {"Alice", "Bob", "Charlie", "Diana", "Eve"};
    private static final Random RANDOM = new Random();

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(", ");
                    String name = parts[0];
                    long amount = Long.parseLong(parts[1]);
                    return new Transaction(name, amount); // Assuming Transaction is a class you define
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingLong(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return Source.fromProcessor("SimulatedSource", SourceProcessor.builder()
                .<String>flatMap((context, emitter) -> {
                    for (int i = 0; i < 100; i++) {
                        String name = NAMES[RANDOM.nextInt(NAMES.length)];
                        long amount = RANDOM.nextInt(1000) + 1; // Random amount between 1 and 1000
                        emitter.emit(name + ", " + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build());
    }

    // Example Transaction class
    static class Transaction {
        private final String name;
        private final long amount;

        public Transaction(String name, long amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public long getAmount() {
            return amount;
        }
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource")
                .<String>streaming()
                .fillBufferFn((buffer, ctx) -> {
                    String[] names = {"Alice", "Bob", "Charlie", "David"};
                    for (long i = 0; i < 100; i++) {
                        String name = names[(int) (i % names.length)];
                        double amount = Math.random() * 100; // Generate random amount
                        buffer.add(name + "," + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}



import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource", ctx -> new SourceProcessor<String>() {
            private final String[] names = {"Alice", "Bob", "Charlie", "David"};
            private long count = 0;

            @Override
            public void process(int ordinal, SourceProcessorContext context) {
                if (count < 100) {
                    String name = names[(int) (count % names.length)];
                    double amount = Math.random() * 100; // Generate random amount
                    context.emit(name + "," + amount); // Emit simulated data
                    count++;
                } else {
                    context.halt(); // Stop after 100 records
                }
            }
        }).build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}


public class TimeUtil {

    public static String convertMillisToHMS(long millis) {
        long seconds = millis / 1000;
        long hours = seconds / 3600;
        seconds %= 3600;
        long minutes = seconds / 60;
        seconds %= 60;

        return String.format("%02d:%02d:%02d", hours, minutes, seconds);
    }

    public static void main(String[] args) {
        long milliseconds = 3661000; // Example: 1 hour, 1 minute, and 1 second
        String formattedTime = convertMillisToHMS(milliseconds);
        System.out.println(formattedTime); // Output: 01:01:01
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;

import java.time.Instant;
import java.time.LocalTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;

public class EventWithTimeLoggingExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Formatter to display time as HH:mm:ss.SSS
        DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("HH:mm:ss.SSS");

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Use the map journal as a stream source
        pipeline.readFrom(Sources.mapJournal("myMap"))
                .filter(event -> event.getType() == com.hazelcast.map.journal.EventJournalMapEvent.Type.ADDED)
                .withTimestamps(event -> {
                    long timestampMillis = System.currentTimeMillis(); // Extract the timestamp

                    // Convert epoch millis to LocalTime (only hours, minutes, seconds, milliseconds)
                    LocalTime eventTime = Instant.ofEpochMilli(timestampMillis)
                                                 .atZone(ZoneId.systemDefault())
                                                 .toLocalTime();

                    // Log only the time part (HH:mm:ss.SSS)
                    System.out.println("Event: " + event + ", Time: " + eventTime.format(timeFormatter));

                    return timestampMillis; // Return the original timestamp for Jet's internal use
                }, 0)  // Zero lag time
                .writeTo(Sinks.logger());

        // Execute the pipeline
        jet.newJob(pipeline);

        // Simulate map updates in another thread or environment (not shown here)
    }
}




The Directed Acyclic Graph (DAG) you see here represents the execution plan for the Hazelcast Jet job named "ExpenseAggregationJob." A DAG is a flowchart-like structure that shows the various stages of the job execution, including data sources, transformation steps, and output sinks, along with their relationships.

Here’s a breakdown of what you are seeing in the DAG and its attributes:

1. **"file-source" [localParallelism=1]**:
   - This is the node in the DAG where data is read from the file. The attribute `localParallelism=1` indicates that this operation is not parallelized (it runs on a single thread on each node).

2. **"file-source-add-timestamps" [localParallelism=1]**:
   - After reading the file, timestamps are added to the records. This is still running with `localParallelism=1`, meaning only one thread per node handles this.

3. **"map" [localParallelism=8]**:
   - This is a transformation operation (possibly filtering, mapping, or similar). Here, parallelism is increased to 8, meaning that 8 threads will process the data.

4. **"sliding-window-prepare" and "sliding-window" [localParallelism=8]**:
   - These represent operations preparing for and performing a sliding window aggregation. Sliding windows are commonly used for time-based aggregations, and with `localParallelism=8`, multiple threads will handle the windowing and aggregation of data.

5. **"loggerSink" [localParallelism=1]**:
   - The final step, which logs the output. This only uses one thread.

### Sequence of Execution
- The DAG flow is indicated by arrows (`->`) that show the sequence of execution:
   - `file-source -> file-source-add-timestamps`: Data is first read from the file, and timestamps are added.
   - `file-source-add-timestamps -> map`: The data, now with timestamps, is passed to the `map` step, where some transformation happens.
   - `map -> sliding-window-prepare -> sliding-window`: After mapping, the data is processed through a sliding window preparation stage, then aggregated over the sliding window.
   - `sliding-window -> loggerSink`: Finally, the aggregated data is passed to the logger sink.

### Where is the Data?
- **Data from the File**: The node `"file-source"` is where your file data is read.
- **Data Transformation**: As the DAG progresses from `"file-source"` to `"map"` and later to `"sliding-window"`, the data is transformed and aggregated.
- **Data Flow**: The arrows in the DAG show how the data moves from one step to another.

### Attributes in the DAG
- **localParallelism**: This attribute indicates how many threads will execute a particular task on each node in the Hazelcast Jet cluster.
- **queueSize**: This attribute specifies the size of the queue between stages. For instance, `queueSize=1024` indicates that the system will buffer up to 1024 items between these stages before processing the next batch.

### Summary of the Flow:
1. **File Reading** (`file-source`) -> 
2. **Add Timestamps** (`file-source-add-timestamps`) -> 
3. **Transformation/Mapping** (`map`) -> 
4. **Window Preparation & Aggregation** (`sliding-window-prepare`, `sliding-window`) -> 
5. **Logging the Results** (`loggerSink`).

In this DAG, your data is being read from a file, transformed, aggregated over a sliding window, and finally logged. The `map` and `sliding-window` stages are running with high parallelism to speed up the job.




===========


The log message you see:

```
Members {size: 1, ver: 1} [
    Member 127.0.0.1 -
]
```

indicates that Hazelcast has successfully started with **one member** in the cluster, and that member's IP address is `127.0.0.1` (localhost). Let’s break it down in more detail:

### Components of the Message

1. **Members {size: 1, ver: 1}**:
   - **size: 1**: This shows the number of members (or nodes) currently in the Hazelcast cluster. In this case, the size is 1, meaning there is only **one node** (your current instance) in the cluster.
   - **ver: 1**: This indicates the version of the member list. Hazelcast assigns a version to the list of members. When a member joins or leaves, the version gets incremented, indicating a change in the cluster membership. In this case, the version is 1, meaning it's the initial cluster state with just one member.

2. **Member 127.0.0.1 -**:
   - **127.0.0.1**: This is the IP address of the Hazelcast member. Since you are running this on your local machine, the member is bound to `localhost` (i.e., `127.0.0.1`). In production, this would be the real IP address of the server or container where Hazelcast is running.
   - **-**: Sometimes Hazelcast adds an optional unique identifier for each member here, but in this case, no identifier is shown. In distributed setups, this would display a UUID assigned to the member for easier identification.

### What This Means for Your Setup
- You are running a **single-node Hazelcast cluster** locally on your machine (`127.0.0.1`). 
- The message indicates that this node is the **only active member** of the Hazelcast cluster.
- The cluster is running, but since it's a single-node setup, there is no distribution of data across multiple nodes, and no high availability is present.

### In Production

In a production environment, the configuration and scale are very different, and this message will have more significance.

#### 1. **Cluster Size**
   In a typical production deployment, you will usually have multiple Hazelcast instances forming a cluster. The cluster size could be 3, 5, or more, depending on your requirements for **high availability**, **fault tolerance**, and **scalability**.

   Example log with a 3-member cluster:
   ```
   Members {size: 3, ver: 3} [
       Member 192.168.1.1 -
       Member 192.168.1.2 -
       Member 192.168.1.3 -
   ]
   ```

   - The **size: 3** means three nodes are part of the cluster.
   - **ver: 3** indicates that there were three changes to the membership (e.g., all three nodes joined the cluster).

#### 2. **Distributed Data**
   In a multi-member cluster, Hazelcast will distribute the data across all members using its **partitioning** mechanism. Each member (node) is responsible for holding a subset of the data. For example:
   - If you have 3 members, each member might hold 1/3 of the data.
   - Data is automatically replicated to other members for fault tolerance. If one member goes down, its data can still be accessed from the replicas on other members.

#### 3. **Resilience and Scalability**
   - **High Availability**: If you have multiple members, the cluster is resilient to failures. If one node fails, the others continue to serve requests.
   - **Horizontal Scalability**: You can add more members to the cluster to handle larger datasets and increase processing power. When a new member joins, Hazelcast will redistribute the data among all the members.

#### 4. **Cluster Management**
   In production, you may also configure Hazelcast with advanced networking features like:
   - **TCP/IP or Multicast**: To enable members to discover each other on different machines.
   - **WAN Replication**: For clusters spread across data centers.
   - **SSL/TLS**: For secure communication between members.
   - **Custom Partitioning**: To control how data is partitioned across members.

#### 5. **UUID for Identification**
   In production, each member is typically assigned a **UUID**. This UUID is useful for identifying a specific node in a larger cluster.

   Example:
   ```
   Members {size: 3, ver: 3} [
       Member 192.168.1.1 - 90bb25c3-308f-45e2-b58d-2b91bff2469b
       Member 192.168.1.2 - 56fe456a-308e-49fd-8c5e-cdef019b3759
       Member 192.168.1.3 - a05e2fd7-308d-4567-8c5f-2911ff18729f
   ]
   ```

   Here, each member has a unique identifier (`UUID`), which can be useful for troubleshooting and ensuring data replication is occurring correctly.

### Cluster Management in Production

In a production environment, you should also consider:
- **Monitoring Tools**: Hazelcast provides management tools like **Hazelcast Management Center**, which allows you to monitor cluster status, memory usage, data distribution, etc.
- **Scaling**: You can scale the cluster by adding or removing nodes dynamically, and Hazelcast will handle rebalancing data automatically.
- **Data Persistence**: While Hazelcast stores data in memory for speed, it also supports **persistence** using disk storage for failover scenarios.

### Summary
In production:
- **Cluster Size** will reflect the number of active nodes.
- **Version** tracks changes in cluster membership.
- **Data Distribution** will ensure that each node holds part of the data and replicates it for fault tolerance.
- **UUIDs** uniquely identify each member.

Running Hazelcast in a **single-node** setup like your local environment is ideal for development and testing, but in production, a **multi-member cluster** provides horizontal scalability, high availability, and data resilience.



