import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.Sources;

import java.util.Random;
import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    private static final String[] NAMES = {"Alice", "Bob", "Charlie", "Diana", "Eve"};
    private static final Random RANDOM = new Random();

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(", ");
                    String name = parts[0];
                    long amount = Long.parseLong(parts[1]);
                    return new Transaction(name, amount); // Assuming Transaction is a class you define
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingLong(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return Source.fromProcessor("SimulatedSource", SourceProcessor.builder()
                .<String>flatMap((context, emitter) -> {
                    for (int i = 0; i < 100; i++) {
                        String name = NAMES[RANDOM.nextInt(NAMES.length)];
                        long amount = RANDOM.nextInt(1000) + 1; // Random amount between 1 and 1000
                        emitter.emit(name + ", " + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build());
    }

    // Example Transaction class
    static class Transaction {
        private final String name;
        private final long amount;

        public Transaction(String name, long amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public long getAmount() {
            return amount;
        }
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource")
                .<String>streaming()
                .fillBufferFn((buffer, ctx) -> {
                    String[] names = {"Alice", "Bob", "Charlie", "David"};
                    for (long i = 0; i < 100; i++) {
                        String name = names[(int) (i % names.length)];
                        double amount = Math.random() * 100; // Generate random amount
                        buffer.add(name + "," + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}



import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource", ctx -> new SourceProcessor<String>() {
            private final String[] names = {"Alice", "Bob", "Charlie", "David"};
            private long count = 0;

            @Override
            public void process(int ordinal, SourceProcessorContext context) {
                if (count < 100) {
                    String name = names[(int) (count % names.length)];
                    double amount = Math.random() * 100; // Generate random amount
                    context.emit(name + "," + amount); // Emit simulated data
                    count++;
                } else {
                    context.halt(); // Stop after 100 records
                }
            }
        }).build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}


public class TimeUtil {

    public static String convertMillisToHMS(long millis) {
        long seconds = millis / 1000;
        long hours = seconds / 3600;
        seconds %= 3600;
        long minutes = seconds / 60;
        seconds %= 60;

        return String.format("%02d:%02d:%02d", hours, minutes, seconds);
    }

    public static void main(String[] args) {
        long milliseconds = 3661000; // Example: 1 hour, 1 minute, and 1 second
        String formattedTime = convertMillisToHMS(milliseconds);
        System.out.println(formattedTime); // Output: 01:01:01
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;

import java.time.Instant;
import java.time.LocalTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;

public class EventWithTimeLoggingExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Formatter to display time as HH:mm:ss.SSS
        DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("HH:mm:ss.SSS");

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Use the map journal as a stream source
        pipeline.readFrom(Sources.mapJournal("myMap"))
                .filter(event -> event.getType() == com.hazelcast.map.journal.EventJournalMapEvent.Type.ADDED)
                .withTimestamps(event -> {
                    long timestampMillis = System.currentTimeMillis(); // Extract the timestamp

                    // Convert epoch millis to LocalTime (only hours, minutes, seconds, milliseconds)
                    LocalTime eventTime = Instant.ofEpochMilli(timestampMillis)
                                                 .atZone(ZoneId.systemDefault())
                                                 .toLocalTime();

                    // Log only the time part (HH:mm:ss.SSS)
                    System.out.println("Event: " + event + ", Time: " + eventTime.format(timeFormatter));

                    return timestampMillis; // Return the original timestamp for Jet's internal use
                }, 0)  // Zero lag time
                .writeTo(Sinks.logger());

        // Execute the pipeline
        jet.newJob(pipeline);

        // Simulate map updates in another thread or environment (not shown here)
    }
}




The Directed Acyclic Graph (DAG) you see here represents the execution plan for the Hazelcast Jet job named "ExpenseAggregationJob." A DAG is a flowchart-like structure that shows the various stages of the job execution, including data sources, transformation steps, and output sinks, along with their relationships.

Here’s a breakdown of what you are seeing in the DAG and its attributes:

1. **"file-source" [localParallelism=1]**:
   - This is the node in the DAG where data is read from the file. The attribute `localParallelism=1` indicates that this operation is not parallelized (it runs on a single thread on each node).

2. **"file-source-add-timestamps" [localParallelism=1]**:
   - After reading the file, timestamps are added to the records. This is still running with `localParallelism=1`, meaning only one thread per node handles this.

3. **"map" [localParallelism=8]**:
   - This is a transformation operation (possibly filtering, mapping, or similar). Here, parallelism is increased to 8, meaning that 8 threads will process the data.

4. **"sliding-window-prepare" and "sliding-window" [localParallelism=8]**:
   - These represent operations preparing for and performing a sliding window aggregation. Sliding windows are commonly used for time-based aggregations, and with `localParallelism=8`, multiple threads will handle the windowing and aggregation of data.

5. **"loggerSink" [localParallelism=1]**:
   - The final step, which logs the output. This only uses one thread.

### Sequence of Execution
- The DAG flow is indicated by arrows (`->`) that show the sequence of execution:
   - `file-source -> file-source-add-timestamps`: Data is first read from the file, and timestamps are added.
   - `file-source-add-timestamps -> map`: The data, now with timestamps, is passed to the `map` step, where some transformation happens.
   - `map -> sliding-window-prepare -> sliding-window`: After mapping, the data is processed through a sliding window preparation stage, then aggregated over the sliding window.
   - `sliding-window -> loggerSink`: Finally, the aggregated data is passed to the logger sink.

### Where is the Data?
- **Data from the File**: The node `"file-source"` is where your file data is read.
- **Data Transformation**: As the DAG progresses from `"file-source"` to `"map"` and later to `"sliding-window"`, the data is transformed and aggregated.
- **Data Flow**: The arrows in the DAG show how the data moves from one step to another.

### Attributes in the DAG
- **localParallelism**: This attribute indicates how many threads will execute a particular task on each node in the Hazelcast Jet cluster.
- **queueSize**: This attribute specifies the size of the queue between stages. For instance, `queueSize=1024` indicates that the system will buffer up to 1024 items between these stages before processing the next batch.

### Summary of the Flow:
1. **File Reading** (`file-source`) -> 
2. **Add Timestamps** (`file-source-add-timestamps`) -> 
3. **Transformation/Mapping** (`map`) -> 
4. **Window Preparation & Aggregation** (`sliding-window-prepare`, `sliding-window`) -> 
5. **Logging the Results** (`loggerSink`).

In this DAG, your data is being read from a file, transformed, aggregated over a sliding window, and finally logged. The `map` and `sliding-window` stages are running with high parallelism to speed up the job.



