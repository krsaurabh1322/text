import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.Sources;

import java.util.Random;
import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    private static final String[] NAMES = {"Alice", "Bob", "Charlie", "Diana", "Eve"};
    private static final Random RANDOM = new Random();

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(", ");
                    String name = parts[0];
                    long amount = Long.parseLong(parts[1]);
                    return new Transaction(name, amount); // Assuming Transaction is a class you define
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingLong(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return Source.fromProcessor("SimulatedSource", SourceProcessor.builder()
                .<String>flatMap((context, emitter) -> {
                    for (int i = 0; i < 100; i++) {
                        String name = NAMES[RANDOM.nextInt(NAMES.length)];
                        long amount = RANDOM.nextInt(1000) + 1; // Random amount between 1 and 1000
                        emitter.emit(name + ", " + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build());
    }

    // Example Transaction class
    static class Transaction {
        private final String name;
        private final long amount;

        public Transaction(String name, long amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public long getAmount() {
            return amount;
        }
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource")
                .<String>streaming()
                .fillBufferFn((buffer, ctx) -> {
                    String[] names = {"Alice", "Bob", "Charlie", "David"};
                    for (long i = 0; i < 100; i++) {
                        String name = names[(int) (i % names.length)];
                        double amount = Math.random() * 100; // Generate random amount
                        buffer.add(name + "," + amount); // Emit simulated data
                        Thread.sleep(100); // Simulate some delay between emissions
                    }
                })
                .build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}



import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;
import com.hazelcast.jet.pipeline.SourceBuilder;

import java.util.concurrent.TimeUnit;

public class TumblingWindowExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Create a source simulating data stream
        StreamSource<String> source = createSimulatedSource();

        // Add the source to the pipeline
        pipeline.readFrom(source)
                .map(value -> {
                    String[] parts = value.split(",");
                    String name = parts[0];
                    double amount = Double.parseDouble(parts[1]);
                    return new Transaction(name, amount);
                })
                .groupingKey(Transaction::getName) // Group by name
                .window(WindowDefinition.tumbling(5, TimeUnit.SECONDS)) // Tumbling window of 5 seconds
                .aggregate(AggregateOperations.summingDouble(Transaction::getAmount)) // Sum the amounts
                .writeTo(Sinks.logger()); // Log the results to the console

        // Execute the pipeline
        jet.newJob(pipeline);
    }

    private static StreamSource<String> createSimulatedSource() {
        return SourceBuilder.stream("SimulatedSource", ctx -> new SourceProcessor<String>() {
            private final String[] names = {"Alice", "Bob", "Charlie", "David"};
            private long count = 0;

            @Override
            public void process(int ordinal, SourceProcessorContext context) {
                if (count < 100) {
                    String name = names[(int) (count % names.length)];
                    double amount = Math.random() * 100; // Generate random amount
                    context.emit(name + "," + amount); // Emit simulated data
                    count++;
                } else {
                    context.halt(); // Stop after 100 records
                }
            }
        }).build();
    }

    // Transaction class to hold name and amount
    static class Transaction {
        private final String name;
        private final double amount;

        public Transaction(String name, double amount) {
            this.name = name;
            this.amount = amount;
        }

        public String getName() {
            return name;
        }

        public double getAmount() {
            return amount;
        }
    }
}


public class TimeUtil {

    public static String convertMillisToHMS(long millis) {
        long seconds = millis / 1000;
        long hours = seconds / 3600;
        seconds %= 3600;
        long minutes = seconds / 60;
        seconds %= 60;

        return String.format("%02d:%02d:%02d", hours, minutes, seconds);
    }

    public static void main(String[] args) {
        long milliseconds = 3661000; // Example: 1 hour, 1 minute, and 1 second
        String formattedTime = convertMillisToHMS(milliseconds);
        System.out.println(formattedTime); // Output: 01:01:01
    }
}




import com.hazelcast.jet.Jet;
import com.hazelcast.jet.JetInstance;
import com.hazelcast.jet.pipeline.*;

import java.time.Instant;
import java.time.LocalTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;

public class EventWithTimeLoggingExample {

    public static void main(String[] args) {
        JetInstance jet = Jet.newJetInstance();

        // Formatter to display time as HH:mm:ss.SSS
        DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("HH:mm:ss.SSS");

        // Create a pipeline
        Pipeline pipeline = Pipeline.create();

        // Use the map journal as a stream source
        pipeline.readFrom(Sources.mapJournal("myMap"))
                .filter(event -> event.getType() == com.hazelcast.map.journal.EventJournalMapEvent.Type.ADDED)
                .withTimestamps(event -> {
                    long timestampMillis = System.currentTimeMillis(); // Extract the timestamp

                    // Convert epoch millis to LocalTime (only hours, minutes, seconds, milliseconds)
                    LocalTime eventTime = Instant.ofEpochMilli(timestampMillis)
                                                 .atZone(ZoneId.systemDefault())
                                                 .toLocalTime();

                    // Log only the time part (HH:mm:ss.SSS)
                    System.out.println("Event: " + event + ", Time: " + eventTime.format(timeFormatter));

                    return timestampMillis; // Return the original timestamp for Jet's internal use
                }, 0)  // Zero lag time
                .writeTo(Sinks.logger());

        // Execute the pipeline
        jet.newJob(pipeline);

        // Simulate map updates in another thread or environment (not shown here)
    }
}




The Directed Acyclic Graph (DAG) you see here represents the execution plan for the Hazelcast Jet job named "ExpenseAggregationJob." A DAG is a flowchart-like structure that shows the various stages of the job execution, including data sources, transformation steps, and output sinks, along with their relationships.

Here’s a breakdown of what you are seeing in the DAG and its attributes:

1. **"file-source" [localParallelism=1]**:
   - This is the node in the DAG where data is read from the file. The attribute `localParallelism=1` indicates that this operation is not parallelized (it runs on a single thread on each node).

2. **"file-source-add-timestamps" [localParallelism=1]**:
   - After reading the file, timestamps are added to the records. This is still running with `localParallelism=1`, meaning only one thread per node handles this.

3. **"map" [localParallelism=8]**:
   - This is a transformation operation (possibly filtering, mapping, or similar). Here, parallelism is increased to 8, meaning that 8 threads will process the data.

4. **"sliding-window-prepare" and "sliding-window" [localParallelism=8]**:
   - These represent operations preparing for and performing a sliding window aggregation. Sliding windows are commonly used for time-based aggregations, and with `localParallelism=8`, multiple threads will handle the windowing and aggregation of data.

5. **"loggerSink" [localParallelism=1]**:
   - The final step, which logs the output. This only uses one thread.

### Sequence of Execution
- The DAG flow is indicated by arrows (`->`) that show the sequence of execution:
   - `file-source -> file-source-add-timestamps`: Data is first read from the file, and timestamps are added.
   - `file-source-add-timestamps -> map`: The data, now with timestamps, is passed to the `map` step, where some transformation happens.
   - `map -> sliding-window-prepare -> sliding-window`: After mapping, the data is processed through a sliding window preparation stage, then aggregated over the sliding window.
   - `sliding-window -> loggerSink`: Finally, the aggregated data is passed to the logger sink.

### Where is the Data?
- **Data from the File**: The node `"file-source"` is where your file data is read.
- **Data Transformation**: As the DAG progresses from `"file-source"` to `"map"` and later to `"sliding-window"`, the data is transformed and aggregated.
- **Data Flow**: The arrows in the DAG show how the data moves from one step to another.

### Attributes in the DAG
- **localParallelism**: This attribute indicates how many threads will execute a particular task on each node in the Hazelcast Jet cluster.
- **queueSize**: This attribute specifies the size of the queue between stages. For instance, `queueSize=1024` indicates that the system will buffer up to 1024 items between these stages before processing the next batch.

### Summary of the Flow:
1. **File Reading** (`file-source`) -> 
2. **Add Timestamps** (`file-source-add-timestamps`) -> 
3. **Transformation/Mapping** (`map`) -> 
4. **Window Preparation & Aggregation** (`sliding-window-prepare`, `sliding-window`) -> 
5. **Logging the Results** (`loggerSink`).

In this DAG, your data is being read from a file, transformed, aggregated over a sliding window, and finally logged. The `map` and `sliding-window` stages are running with high parallelism to speed up the job.




===========


The log message you see:

```
Members {size: 1, ver: 1} [
    Member 127.0.0.1 -
]
```

indicates that Hazelcast has successfully started with **one member** in the cluster, and that member's IP address is `127.0.0.1` (localhost). Let’s break it down in more detail:

### Components of the Message

1. **Members {size: 1, ver: 1}**:
   - **size: 1**: This shows the number of members (or nodes) currently in the Hazelcast cluster. In this case, the size is 1, meaning there is only **one node** (your current instance) in the cluster.
   - **ver: 1**: This indicates the version of the member list. Hazelcast assigns a version to the list of members. When a member joins or leaves, the version gets incremented, indicating a change in the cluster membership. In this case, the version is 1, meaning it's the initial cluster state with just one member.

2. **Member 127.0.0.1 -**:
   - **127.0.0.1**: This is the IP address of the Hazelcast member. Since you are running this on your local machine, the member is bound to `localhost` (i.e., `127.0.0.1`). In production, this would be the real IP address of the server or container where Hazelcast is running.
   - **-**: Sometimes Hazelcast adds an optional unique identifier for each member here, but in this case, no identifier is shown. In distributed setups, this would display a UUID assigned to the member for easier identification.

### What This Means for Your Setup
- You are running a **single-node Hazelcast cluster** locally on your machine (`127.0.0.1`). 
- The message indicates that this node is the **only active member** of the Hazelcast cluster.
- The cluster is running, but since it's a single-node setup, there is no distribution of data across multiple nodes, and no high availability is present.

### In Production

In a production environment, the configuration and scale are very different, and this message will have more significance.

#### 1. **Cluster Size**
   In a typical production deployment, you will usually have multiple Hazelcast instances forming a cluster. The cluster size could be 3, 5, or more, depending on your requirements for **high availability**, **fault tolerance**, and **scalability**.

   Example log with a 3-member cluster:
   ```
   Members {size: 3, ver: 3} [
       Member 192.168.1.1 -
       Member 192.168.1.2 -
       Member 192.168.1.3 -
   ]
   ```

   - The **size: 3** means three nodes are part of the cluster.
   - **ver: 3** indicates that there were three changes to the membership (e.g., all three nodes joined the cluster).

#### 2. **Distributed Data**
   In a multi-member cluster, Hazelcast will distribute the data across all members using its **partitioning** mechanism. Each member (node) is responsible for holding a subset of the data. For example:
   - If you have 3 members, each member might hold 1/3 of the data.
   - Data is automatically replicated to other members for fault tolerance. If one member goes down, its data can still be accessed from the replicas on other members.

#### 3. **Resilience and Scalability**
   - **High Availability**: If you have multiple members, the cluster is resilient to failures. If one node fails, the others continue to serve requests.
   - **Horizontal Scalability**: You can add more members to the cluster to handle larger datasets and increase processing power. When a new member joins, Hazelcast will redistribute the data among all the members.

#### 4. **Cluster Management**
   In production, you may also configure Hazelcast with advanced networking features like:
   - **TCP/IP or Multicast**: To enable members to discover each other on different machines.
   - **WAN Replication**: For clusters spread across data centers.
   - **SSL/TLS**: For secure communication between members.
   - **Custom Partitioning**: To control how data is partitioned across members.

#### 5. **UUID for Identification**
   In production, each member is typically assigned a **UUID**. This UUID is useful for identifying a specific node in a larger cluster.

   Example:
   ```
   Members {size: 3, ver: 3} [
       Member 192.168.1.1 - 90bb25c3-308f-45e2-b58d-2b91bff2469b
       Member 192.168.1.2 - 56fe456a-308e-49fd-8c5e-cdef019b3759
       Member 192.168.1.3 - a05e2fd7-308d-4567-8c5f-2911ff18729f
   ]
   ```

   Here, each member has a unique identifier (`UUID`), which can be useful for troubleshooting and ensuring data replication is occurring correctly.

### Cluster Management in Production

In a production environment, you should also consider:
- **Monitoring Tools**: Hazelcast provides management tools like **Hazelcast Management Center**, which allows you to monitor cluster status, memory usage, data distribution, etc.
- **Scaling**: You can scale the cluster by adding or removing nodes dynamically, and Hazelcast will handle rebalancing data automatically.
- **Data Persistence**: While Hazelcast stores data in memory for speed, it also supports **persistence** using disk storage for failover scenarios.

### Summary
In production:
- **Cluster Size** will reflect the number of active nodes.
- **Version** tracks changes in cluster membership.
- **Data Distribution** will ensure that each node holds part of the data and replicates it for fault tolerance.
- **UUIDs** uniquely identify each member.

Running Hazelcast in a **single-node** setup like your local environment is ideal for development and testing, but in production, a **multi-member cluster** provides horizontal scalability, high availability, and data resilience.


≠===========

In Hazelcast 5.x and later, you can submit jobs through the `JetService` and also manage the lifecycle of both the server and the jobs. To handle shutting down the Hazelcast cluster or server and managing job lifecycles, follow these steps.

### 1. **Submitting a Job Using JetService**

First, get the `JetService` from your Hazelcast instance. The `JetService` is used to submit, manage, and monitor Jet jobs.

Here’s how to submit a job:

```java
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.jet.JetService;
import com.hazelcast.jet.pipeline.Pipeline;

public class JetJobSubmission {

    public static void main(String[] args) {
        // Assume you already have a HazelcastInstance running (server)
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        
        // Get the JetService from the Hazelcast instance
        JetService jetService = hazelcastInstance.getJet();
        
        // Create a pipeline (this is just an example)
        Pipeline pipeline = Pipeline.create();
        // Define the pipeline logic here (data sources, transformations, sinks)
        
        // Submit the pipeline as a Jet job
        jetService.newJob(pipeline);

        System.out.println("Jet job submitted.");
    }
}
```

### 2. **Shutting Down Hazelcast and JetService**

To gracefully shut down Hazelcast and any running Jet jobs, you can simply use the `shutdown()` method on the `HazelcastInstance`. This will stop the server and shut down all the resources associated with it, including Jet jobs:

```java
import com.hazelcast.core.HazelcastInstance;

public class JetShutdown {

    public static void main(String[] args) {
        // Start Hazelcast instance (as a server)
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        
        // Get the JetService
        JetService jetService = hazelcastInstance.getJet();
        
        // Perform Jet job submission or other logic
        // ...

        // Shutdown the Hazelcast instance (and JetService)
        hazelcastInstance.shutdown();

        System.out.println("Hazelcast server and Jet service have been shut down.");
    }
}
```

### 3. **Graceful Job Cancellation Before Shutdown**

If you want to cancel or gracefully finish jobs before shutting down the Hazelcast server, you can retrieve the jobs via the `JetService` and cancel or complete them as needed:

```java
import com.hazelcast.jet.Job;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.jet.JetService;

import java.util.List;

public class JobManagement {

    public static void main(String[] args) {
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        JetService jetService = hazelcastInstance.getJet();

        // Get all running jobs
        List<Job> jobs = jetService.getJobs();

        // Cancel all running jobs before shutdown
        for (Job job : jobs) {
            System.out.println("Cancelling job: " + job.getName());
            job.cancel();
        }

        // Now shut down Hazelcast and JetService
        hazelcastInstance.shutdown();
    }
}
```

### 4. **Job Lifecycle Management**
- `job.cancel()`: Cancels the job. It doesn't guarantee that the job will terminate instantly, but it will stop once it reaches a safe stopping point.
- `job.join()`: Waits for the job to complete.
- `job.restart()`: Restarts the job if it has failed or been canceled.

### 5. **Shutting Down a Jet Client**

If you’re using a Jet client (`JetClient`), simply call the `shutdown()` method on the client when you're done:

```java
import com.hazelcast.jet.JetClient;

public class JetClientShutdown {

    public static void main(String[] args) {
        // Assume JetClient is running
        JetClient jetClient = Jet.newJetClient();
        
        // Submit job logic here if needed
        // ...

        // Shutdown the JetClient when done
        jetClient.shutdown();
        System.out.println("JetClient has been shut down.");
    }
}
```

### Summary:
- Use `jetService.newJob(pipeline)` to submit a job.
- Use `hazelcastInstance.shutdown()` to gracefully shut down Hazelcast, which also shuts down Jet jobs.
- Optionally, retrieve and manage jobs using `jetService.getJobs()`, and call methods like `cancel()`, `join()`, or `restart()` before shutting down the server.


============[=[


Certainly! Here's the content for each slide based on the outline.

---

### **Slide 1: Title Slide**
- **Title**: *Hazelcast 5: Distributed Cache, In-Memory Data Grid, and Real-Time Stream Processing with Jet*
- **Content**: 
  - Your name, company, and presentation date.
  - "In this presentation, we will explore the features of Hazelcast as a distributed in-memory data grid and its integration with Jet for real-time, low-latency stream processing."

---

### **Slide 2: Introduction to Hazelcast**
- **Title**: *What is Hazelcast?*
- **Content**:
  - Hazelcast is an open-source, distributed, in-memory data grid (IMDG).
  - It provides a scalable, fault-tolerant, and highly available platform for data storage and processing.
  - **Key features**: Low-latency access to data, replication, partitioning, and high throughput.
  - Widely used for applications requiring distributed caching, session clustering, and real-time analytics.

---

### **Slide 3: Hazelcast as a Distributed Cache**
- **Title**: *Distributed Caching with Hazelcast*
- **Content**:
  - Hazelcast serves as a **distributed cache** that allows in-memory data storage across multiple nodes.
  - Supports **horizontal scaling** by adding more nodes to the cluster, allowing for larger datasets.
  - **Data replication** ensures high availability, with automatic failover.
  - Use cases: Session caching, web application acceleration, and frequently accessed data storage.

---

### **Slide 4: In-Memory Data Grid Processing**
- **Title**: *Hazelcast as an In-Memory Data Grid*
- **Content**:
  - Hazelcast acts as a full-fledged in-memory data grid, storing data across distributed clusters.
  - Supports **distributed objects** like maps, sets, and queues.
  - Real-time processing capabilities allow execution of computations directly in-memory, reducing latency.
  - **Use cases**: Microservices architecture, distributed computing, real-time monitoring.

---

### **Slide 5: Hazelcast Jet Overview**
- **Title**: *Introduction to Hazelcast Jet*
- **Content**:
  - Hazelcast Jet is integrated into Hazelcast 5, providing a unified platform for both **in-memory data grid (IMDG)** and **stream processing**.
  - Focuses on **distributed batch and stream processing** with low-latency.
  - **Jet pipelines** allow real-time processing and analytics by defining steps to process data from sources, apply transformations, and output results.
  - Simplified architecture: One cluster for both data grid and stream processing.

---

### **Slide 6: Jet Pipelines and Stream Processing**
- **Title**: *Pipelines and Stream Processing in Jet*
- **Content**:
  - Jet provides a **pipeline API** to define **data flows** from sources to sinks.
  - A pipeline consists of **transformations**, such as mapping, filtering, and aggregating data in a distributed way.
  - Jet pipelines can process both **real-time streams** and **batch data**.
  - **Use cases**: Streaming analytics, event-driven applications, IoT data processing.

---

### **Slide 7: Stream Sources in Jet**
- **Title**: *Data Sources in Jet Pipelines*
- **Content**:
  - Jet pipelines can consume data from a variety of sources: **Hazelcast maps**, **Kafka**, **databases**, **file systems**, and **message queues**.
  - A file stream can simulate real-time data processing by reading the data line-by-line.
  - Example: Pipeline that reads from a file and processes it in real-time.
  - This flexibility makes Jet suitable for diverse use cases like log monitoring and financial transactions.

---

### **Slide 8: Hazelcast Jet Pipelines Example**
- **Title**: *Example of Jet Pipeline*
- **Content**:
  - A simple example of creating a Jet pipeline:
    1. **Source**: Read from a file or a queue.
    2. **Transformations**: Apply filters, transformations, and aggregations.
    3. **Sink**: Write results to a file, Hazelcast map, or database.
  - **Diagram**: Display a pipeline with stages of data processing.
  - **Code snippet**: Show how to define a Jet pipeline in Java.

---

### **Slide 9: Stream Processing: Compute Operations**
- **Title**: *Compute Operations in Jet Pipelines*
- **Content**:
  - Jet supports a variety of operations to transform data:
    - **map()**: Apply a function to each element.
    - **filter()**: Keep elements that match a condition.
    - **flatMap()**: Flatten nested collections.
    - **reduce()**: Combine elements into a single result.
  - These operations allow for the distributed transformation of data streams.
  - **Example**: Filtering transactions greater than $100 and then mapping them to another format.

---

### **Slide 10: Aggregations in Jet**
- **Title**: *Aggregation in Hazelcast Jet*
- **Content**:
  - Jet supports both **stateless** and **stateful aggregations**:
    - **Stateless**: Simple aggregations like sum, count, or average over a set of events.
    - **Stateful**: Aggregations that maintain state, like running totals or averages over time.
  - Example: Aggregating sales data over a period to get total sales.
  - **Code snippet**: Show how to implement a **sum()** aggregation in a pipeline.

---

### **Slide 11: Windowing Functions in Jet**
- **Title**: *Windowing Functions in Jet*
- **Content**:
  - **Windowing** allows data to be grouped based on time intervals for aggregation:
    - **Tumbling windows**: Fixed, non-overlapping windows.
    - **Sliding windows**: Overlapping windows, allowing incremental updates.
    - **Session windows**: Dynamically sized windows based on gaps in data.
    - **Global windows**: Aggregate over the entire data stream.
  - Windowing is crucial for real-time analytics and time-based aggregations.

---

### **Slide 12: Jet Windowing Example**
- **Title**: *Example: Sliding Window Aggregation*
- **Content**:
  - A sliding window computes results for overlapping time intervals.
  - **Diagram**: Show how data is grouped into overlapping windows.
  - **Example**: Sliding window that calculates the sum of expenses in the last minute.
  - **Code snippet**: Show a sliding window aggregation in a pipeline.

---

### **Slide 13: Jet SQL for Stream Processing**
- **Title**: *Using SQL in Hazelcast Jet*
- **Content**:
  - Jet allows querying streams with **Jet SQL**, a SQL-like syntax for real-time data.
  - You can run SQL queries over real-time streams just like databases.
  - Example: `SELECT SUM(amount) FROM transactions WHERE time >= NOW() - INTERVAL '1' MINUTE`.
  - Simplifies the task of working with streams using familiar SQL syntax.

---

### **Slide 14: Real-Time Processing Demo**
- **Title**: *Live Demo: Real-Time Data Pipeline*
- **Content**:
  - **Setup**: A live demo showing real-time stream processing using Jet.
  - **Pipeline**: Simulate a stream of transactions being read from a file.
  - **Aggregation**: Calculate and display running totals in real-time using sliding windows.
  - **Goal**: Demonstrate the dynamic recalculation of results as new data flows through the pipeline.

---

### **Slide 15: Conclusion and Q&A**
- **Title**: *Conclusion & Questions*
- **Content**:
  - **Summary**: Hazelcast 5 integrates Jet's real-time stream processing with distributed in-memory caching.
  - **Benefits**: Real-time, low-latency analytics; distributed and scalable architecture.
  - Open the floor for any questions from the audience.

---

These slides will help you present a clear, concise, and technical overview of Hazelcast 5 and Jet for stream processing, along with a practical demo. Let me know if you need any additional code snippets or clarifications!


==========


Hazelcast is a distributed in-memory computing platform that can function both as a **cache** and as an **in-memory data grid (IMDG)**. While both functionalities are built on top of the same platform, they have distinct differences in their primary use cases, architectures, and performance characteristics. Here's a comparison of Hazelcast as a **cache** vs. an **in-memory data grid**:

---

### **1. Hazelcast as a Cache**

**Definition**:  
A cache is a high-speed data storage layer that temporarily stores frequently accessed data, allowing for quicker access than from the original data source (like a database).

**Use Case**:  
- **Accelerating performance** of applications by reducing access to slower databases or remote services.
- **Temporary storage** of data with typically fast read/write times, but persistence is not a priority.

**Key Features of Hazelcast as a Cache**:
- **Simple key-value store**: You store data in memory with key-value pairs for fast access.
- **Near cache**: Hazelcast provides a near-cache feature, which stores frequently accessed data closer to the application nodes to reduce latency.
- **Eviction policies**: Hazelcast supports various eviction policies such as **Least Recently Used (LRU)** or **Least Frequently Used (LFU)** to automatically remove old or less important data from the cache.
- **TTL (Time to Live)**: Cached entries can have a time-to-live duration, meaning they will be automatically removed after a specified period.
- **Backup**: Data in a cache typically isn't backed up on disk, but Hazelcast allows configuring how many backups (in memory) are stored in case of node failure.
- **Simpler architecture**: Typically involves storing data in-memory on a small number of nodes with simple replication mechanisms, designed for quick retrieval.

**When to Use**:
- For **short-lived, frequently accessed data**.
- To **improve performance** of web applications, microservices, or APIs by avoiding hitting the database or backend systems repeatedly.
- In situations where **data loss** is acceptable, and consistency is not critical.
- For **read-heavy workloads**.

---

### **2. Hazelcast as an In-Memory Data Grid (IMDG)**

**Definition**:  
An in-memory data grid (IMDG) is a distributed, in-memory, key-value store that allows for **scalable, fault-tolerant storage** of large amounts of data across multiple nodes in a cluster. It is used not just for caching but as a primary storage layer with a variety of data structures (maps, queues, sets, etc.).

**Use Case**:  
- Distributed in-memory **data storage** and processing for large datasets in real time.
- **Scaling** applications horizontally by distributing both data and compute tasks across multiple nodes.

**Key Features of Hazelcast as an IMDG**:
- **Data partitioning**: Data is split across the cluster nodes (partitions) for distributed storage, allowing horizontal scaling.
- **Data structures**: Hazelcast IMDG supports rich data structures like distributed maps, sets, lists, queues, and multi-maps.
- **Resilience and fault tolerance**: Data is replicated across multiple nodes, ensuring high availability even if some nodes fail.
- **Data processing**: With features like **Hazelcast Jet**, Hazelcast IMDG can handle real-time stream processing and batch data processing.
- **Persistence**: Unlike a cache, Hazelcast IMDG can persist data to disk (via Hot Restart Store), meaning it can survive a full system restart without data loss.
- **Consistency and partitioning**: Hazelcast offers flexibility between consistency and availability (CAP theorem), allowing users to adjust the trade-offs based on their application needs (e.g., using the `Eventual` or `Strong` consistency model).
- **Querying and indexing**: You can query and index data directly within the grid using SQL-like queries, making Hazelcast IMDG ideal for distributed querying.
- **Transactional support**: Provides distributed transactions and two-phase commit (2PC) mechanisms.
- **Compute Grid**: Besides storing data, Hazelcast IMDG also supports distributing compute tasks across nodes (e.g., map-reduce jobs).

**When to Use**:
- For **large-scale, distributed data storage** that needs to be accessed quickly, frequently, and reliably across a cluster.
- When you need **data redundancy**, with automatic failover in case of node failures.
- For applications that require **distributed computing** with the ability to run in-memory tasks like stream processing and querying.
- To handle **write-heavy workloads** with support for transactions and consistency.
- To support **high throughput, low-latency** real-time applications such as IoT platforms, financial services, and analytics platforms.

---

### **Key Differences Between Hazelcast as a Cache vs. IMDG**:

| Feature                  | Hazelcast as a Cache                        | Hazelcast as an IMDG                        |
|--------------------------|---------------------------------------------|--------------------------------------------|
| **Primary Use**           | Fast, temporary storage for frequently accessed data | Distributed, resilient storage for large datasets with processing capabilities |
| **Data Structure**        | Simple key-value pairs                      | Complex data structures (maps, queues, lists, etc.) |
| **Persistence**           | Typically no persistence                    | Optional persistence via Hot Restart Store |
| **Fault Tolerance**       | Backup copies in memory                     | High availability with replication across nodes |
| **Scalability**           | Limited to a few nodes                      | Horizontally scalable across many nodes |
| **Querying**              | Not the primary use case, key-value lookups | Supports querying with SQL-like syntax and indexing |
| **Transaction Support**   | No transactional guarantees                 | Distributed transactions (2PC) and consistency control |
| **Processing Capability** | Mostly for caching (storing/retrieving data)| Real-time processing with compute capabilities (Hazelcast Jet) |
| **Eviction Policies**     | Supports eviction based on usage (LRU, LFU) | Rarely used due to persistence and distributed nature |
| **Best for**              | Temporary data for performance optimization | Real-time, distributed, fault-tolerant applications with data and compute requirements |

---

### **Which to Choose?**
- **Choose Hazelcast as a Cache** if:
  - You only need to **improve performance** by caching frequently accessed data.
  - Your application primarily reads data and doesn’t require distributed transactions or persistent storage.
  - **Data loss is acceptable** in case of failures.

- **Choose Hazelcast as an IMDG** if:
  - You require a **distributed, resilient, and fault-tolerant storage solution** that can handle large datasets.
  - You need **high availability** and persistence with **low-latency access**.
  - Your application benefits from **distributed computing** and real-time data processing.

In summary, Hazelcast as a **cache** is focused on performance optimization through temporary, fast-access data storage, while Hazelcast as an **IMDG** is designed for distributed data management and processing at scale, with resilience and fault tolerance in mind.


